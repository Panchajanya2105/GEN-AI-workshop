{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8a25adf-7e45-4c2e-9876-7402f7f35654",
   "metadata": {},
   "source": [
    "# **Introduction to NLP**:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb5688d-ebda-41f4-88d4-5bba5ad53964",
   "metadata": {},
   "source": [
    "# Required Libraries and Their Purpose\n",
    "\n",
    "| **Library**       | **Installation Command**     | **Purpose** |\n",
    "|-------------------|----------------------------|-------------|\n",
    "| `nltk`           | `pip install nltk`          | Natural Language Processing (Tokenization, Stopword Removal, Lemmatization) |\n",
    "| `scikit-learn`   | `pip install scikit-learn`  | Machine Learning utilities (TF-IDF, Cosine Similarity) |\n",
    "| `pandas`         | `pip install pandas`        | Data handling and processing |\n",
    "| `numpy`          | `pip install numpy`         | Mathematical computations (used internally by `sklearn`) |\n",
    "\n",
    "## Additional NLTK Downloads\n",
    "\n",
    "After installing `nltk`, run the following commands to download necessary resources:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6729a547-6989-42b6-8e52-13a999f11849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All required libraries are installed successfully!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import sklearn\n",
    "import pandas\n",
    "import numpy\n",
    "\n",
    "print(\"All required libraries are installed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c73916e-8363-448a-aae5-48c87c4bcad7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Natural Language Processing (NLP) is a branch of artificial intelligence that enables machines to understand, interpret, and generate human language. One of the first steps in NLP is text preprocessing, which involves cleaning and preparing text for further analysis. This includes:\n",
    "\n",
    "* Tokenization\n",
    "* Stopword Removal\n",
    "* Stemming\n",
    "* Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcabef9-429d-4a71-9cfc-00cc19c1fd45",
   "metadata": {},
   "source": [
    "### 1. Tokenization\n",
    "Tokenization is the process of breaking a text into smaller pieces, called tokens. These tokens can be words, phrases, or sentences.\n",
    "\n",
    "#### Types of Tokenization\n",
    "\n",
    "1. Word Tokenization: Splitting text into individual words.\n",
    "   \n",
    "2. Sentence Tokenization: Splitting text into sentences.\n",
    "\n",
    "### **Why is Tokenization Important in NLP?**\n",
    "\n",
    "Tokenization is **one of the first steps** in NLP (Natural Language Processing). It helps break down text into smaller parts (tokens), making it easier for computers to understand and analyze language.\n",
    "\n",
    "### **Importance of Tokenization:**\n",
    "\n",
    "1.  **Helps in Understanding Text** – Computers can’t read text like humans. Breaking it into words or sentences makes it easier to process.\n",
    "    \n",
    "2.  **Prepares Data for Analysis** – Many NLP tasks (like sentiment analysis or chatbots) require working with individual words or sentences.\n",
    "    \n",
    "3.  **Improves Machine Learning Models** – Models learn better when the text is structured into meaningful parts.\n",
    "    \n",
    "4.  **Removes Unnecessary Complexity** – Instead of working with a full sentence, analyzing word-by-word makes processing more efficient.\n",
    "    \n",
    "\n",
    "### **Example:**\n",
    "\n",
    "Text:👉 _\"Natural Language Processing is amazing!\"_\n",
    "\n",
    "#### **Word Tokenization:**\n",
    "\n",
    "👉 \\['Natural', 'Language', 'Processing', 'is', 'amazing', '!'\\]\n",
    "\n",
    "#### **Sentence Tokenization:**\n",
    "\n",
    "👉 \\[\"Natural Language Processing is amazing!\"\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61c20f82-77bc-4f9d-8f8f-b172c9045a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', '!', 'How', 'are', 'you', 'doing', 'today', '?']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Hello! How are you doing today?\"\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4045734a-7cab-448a-b00c-1ea642d9fc61",
   "metadata": {},
   "source": [
    "## Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1489d805-03ef-4e8e-9ca4-b83fe6fc6c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello!', 'How are you doing today?', 'I hope you are learning NLP.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"Hello! How are you doing today? I hope you are learning NLP.\"\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea4d2bb-f534-4316-8362-3393f097c833",
   "metadata": {},
   "source": [
    "## 2. Stopword Removal\n",
    "   \n",
    "### **What are Stop Words?**\n",
    "\n",
    "Stop words are common words in a language that do not add much meaning to a sentence and are usually **removed** from text analysis to save space and improve efficiency.\n",
    "\n",
    "### **Example:**\n",
    "\n",
    "In the sentence:👉 _\"I love machine learning and AI.\"_\n",
    "\n",
    "*   Words like **\"I\", \"and\"** are stop words because they don't carry significant meaning.\n",
    "    \n",
    "\n",
    "### **Why Remove Stop Words?**\n",
    "\n",
    "1.  **Reduces Noise** – Makes the text cleaner and more focused.\n",
    "    \n",
    "2.  **Improves Performance** – Helps machine learning models process important words faster.\n",
    "    \n",
    "3.  **Saves Storage & Memory** – Eliminates unnecessary words from data storage.\n",
    "    \n",
    "\n",
    "### **Common Stop Words (English)**\n",
    "\n",
    "*   \"the\", \"is\", \"in\", \"and\", \"to\", \"on\", \"at\", \"for\", \"a\", \"an\", \"of\", \"it\"\n",
    "    \n",
    "\n",
    "### **In NLP (Natural Language Processing)**\n",
    "\n",
    "When working with text data, we often remove stop words to focus on important words that affect the meaning, such as **nouns, verbs, and adjectives**.\n",
    "\n",
    "Would you like to see an example of how to remove stop words using Python? 🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57754b7c-f494-489c-b314-e17ed5a8f374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'between', 'we', 'doesn', 'under', 'about', 'how', 'any', 'such', 'couldn', 'once', 'through', \"doesn't\", 'very', 'should', \"haven't\", 'you', 'or', 'ain', 're', 'll', 'it', \"you've\", 'over', 'most', 'until', 'our', 'from', \"won't\", 'above', 'because', 'few', 'aren', 'won', 'she', 'he', \"shouldn't\", 'them', 'does', \"it's\", 'a', 'am', 'just', 'this', 'that', 'each', 'don', \"weren't\", 'didn', \"needn't\", 'both', 'be', 'when', 'can', 'him', \"you'd\", 'shan', 'own', \"you'll\", 'whom', 'nor', \"aren't\", 'with', 'so', 'yours', 'of', 'there', 'but', 's', 'to', 'up', 'hasn', 'further', 'no', 'have', 'off', 'into', 'having', 'yourself', 'those', 'itself', 'before', 'been', 'do', 'himself', \"don't\", 'now', 'out', 'in', 'as', \"shan't\", 'who', 'what', \"wouldn't\", 'y', 'will', 'hadn', \"mightn't\", 'being', \"you're\", 't', \"didn't\", 'hers', 'all', \"couldn't\", 'below', 'too', 'haven', 'during', 'at', 'shouldn', 'on', 'my', 'd', 'o', 'had', 'down', 'are', 'ourselves', 'i', 'mustn', 'than', 'same', 'is', 'again', 'wouldn', 'yourselves', 'his', 'by', \"wasn't\", 'not', 'some', 'the', 'which', 'more', 'me', \"mustn't\", 've', 'ours', 'doing', 'while', 'where', 'm', 'wasn', 'their', \"she's\", \"isn't\", 'for', 'myself', 'did', 'themselves', 'against', 'isn', 'other', 'mightn', 'and', 'your', \"hasn't\", 'its', 'why', 'after', 'only', 'if', 'were', 'her', \"should've\", 'has', 'they', \"hadn't\", 'was', 'herself', 'theirs', 'then', 'here', 'needn', 'an', 'weren', \"that'll\", 'these', 'ma'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45f55d08-d09f-4192-98d1-805765a597d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['How', 'are', 'you', 'doing']\n"
     ]
    }
   ],
   "source": [
    "text2 = \"Hello! How are you doing today?\"\n",
    "tokens1 = word_tokenize(text2)\n",
    "\n",
    "filtered_words = [word for word in tokens1 if word.lower() in stop_words]\n",
    "print(filtered_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc11ba32-3c85-44a6-a503-56b65e0a2fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', '!', 'today', '?']\n"
     ]
    }
   ],
   "source": [
    "filtered_words = [word for word in tokens1 if word.lower() not in stop_words]\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a58562-7ac3-4194-b568-6e9928135d09",
   "metadata": {},
   "source": [
    "# 3. Stemming\n",
    "\n",
    "### **Why is Stemming Important in NLP?**\n",
    "\n",
    "Stemming helps computers **understand the core meaning of words** by reducing them to their root form. This makes text processing **faster and more efficient** in NLP tasks.\n",
    "\n",
    "### **Importance of Stemming:**\n",
    "\n",
    "1.  **Reduces Word Variations** – Words like _running_, _runs_, _runner_ all reduce to _run_, helping models treat them as the same word.\n",
    "    \n",
    "2.  **Saves Storage & Processing Power** – Instead of storing multiple versions of the same word, we only keep the root word.\n",
    "    \n",
    "3.  **Improves Search & Information Retrieval** – A search for _“connect”_ will also match _“connected”_, _“connecting”_, and _“connection”_.\n",
    "    \n",
    "4.  **Enhances Text Analysis** – Makes NLP models more effective by focusing on meaning rather than word variations.\n",
    "    \n",
    "\n",
    "### **Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd6be013-4b39-486f-a894-a00c937e28db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Search Engine Optimization:\n",
      "   Original: ['running', 'runs', 'runner', 'ran', 'run']\n",
      "   Stemmed:  ['run', 'run', 'runner', 'ran', 'run']\n",
      "\n",
      "🔹 Sentiment Analysis:\n",
      "   Original: ['happily', 'happiness', 'happy', 'unhappily', 'unhappy']\n",
      "   Stemmed:  ['happili', 'happi', 'happi', 'unhappili', 'unhappi']\n",
      "\n",
      "🔹 Chatbot Responses:\n",
      "   Original: ['studying', 'studied', 'study', 'studies']\n",
      "   Stemmed:  ['studi', 'studi', 'studi', 'studi']\n",
      "\n",
      "🔹 Spam Detection:\n",
      "   Original: ['buying', 'bought', 'buys', 'buy']\n",
      "   Stemmed:  ['buy', 'bought', 'buy', 'buy']\n",
      "\n",
      "🔹 Resume Screening:\n",
      "   Original: ['developed', 'developing', 'developer', 'development']\n",
      "   Stemmed:  ['develop', 'develop', 'develop', 'develop']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Initialize the stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Example words for different NLP tasks\n",
    "examples = {\n",
    "    \"Search Engine Optimization\": [\"running\", \"runs\", \"runner\", \"ran\", \"run\"],\n",
    "    \"Sentiment Analysis\": [\"happily\", \"happiness\", \"happy\", \"unhappily\", \"unhappy\"],\n",
    "    \"Chatbot Responses\": [\"studying\", \"studied\", \"study\", \"studies\"],\n",
    "    \"Spam Detection\": [\"buying\", \"bought\", \"buys\", \"buy\"],\n",
    "    \"Resume Screening\": [\"developed\", \"developing\", \"developer\", \"development\"]\n",
    "}\n",
    "\n",
    "# Apply stemming and print results\n",
    "for use_case, words in examples.items():\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    print(f\"\\n🔹 {use_case}:\\n   Original: {words}\\n   Stemmed:  {stemmed_words}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381e536a-1079-435e-b43c-99c83b50265f",
   "metadata": {},
   "source": [
    "# 4. Lemmatization\n",
    "\n",
    "### **Why is Lemmatization Important in NLP?**\n",
    "\n",
    "Lemmatization is **better than stemming** because it reduces words to their meaningful base form (lemma) **without losing meaning**. It considers **grammar, context, and dictionary meanings**, making NLP models more **accurate and natural**.\n",
    "\n",
    "### **Importance of Lemmatization:**\n",
    "\n",
    "1.  **Produces Real Words** – Unlike stemming, which sometimes gives incorrect root forms (_e.g., “studies” → “studi”_), lemmatization gives correct words (_e.g., “studies” → “study”_).\n",
    "    \n",
    "2.  **Better Text Understanding** – Helps NLP applications like **chatbots, search engines, and sentiment analysis** process words correctly.\n",
    "    \n",
    "3.  **Handles Different Word Forms** – Converts **verbs, nouns, and adjectives** to their dictionary forms (e.g., _\"running\"_ → _\"run\"_, _\"mice\"_ → _\"mouse\"_).\n",
    "    \n",
    "4.  **Improves Model Accuracy** – Machine learning models perform better when trained on proper words rather than incorrect stems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19ebf0cb-4494-42bb-b8fe-21b9367a4e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'fly', 'easily', 'fairly', 'connect']\n",
      "\n",
      "🔹 Search Engine Optimization:\n",
      "   Original: ['running', 'runs', 'runner', 'ran', 'run']\n",
      "   Stemmed:  ['running', 'run', 'runner', 'ran', 'run']\n",
      "\n",
      "🔹 Sentiment Analysis:\n",
      "   Original: ['happily', 'happiness', 'happy', 'unhappily', 'unhappy']\n",
      "   Stemmed:  ['happily', 'happiness', 'happy', 'unhappily', 'unhappy']\n",
      "\n",
      "🔹 Chatbot Responses:\n",
      "   Original: ['studying', 'studied', 'study', 'studies']\n",
      "   Stemmed:  ['studying', 'studied', 'study', 'study']\n",
      "\n",
      "🔹 Spam Detection:\n",
      "   Original: ['buying', 'bought', 'buys', 'buy']\n",
      "   Stemmed:  ['buying', 'bought', 'buy', 'buy']\n",
      "\n",
      "🔹 Resume Screening:\n",
      "   Original: ['developed', 'developing', 'developer', 'development']\n",
      "   Stemmed:  ['developed', 'developing', 'developer', 'development']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = [\"running\", \"flies\", \"easily\", \"fairly\", \"connected\"]\n",
    "lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in words]\n",
    "\n",
    "print(lemmatized_words)\n",
    "\n",
    "\n",
    "# Example words for different NLP tasks\n",
    "examples = {\n",
    "    \"Search Engine Optimization\": [\"running\", \"runs\", \"runner\", \"ran\", \"run\"],\n",
    "    \"Sentiment Analysis\": [\"happily\", \"happiness\", \"happy\", \"unhappily\", \"unhappy\"],\n",
    "    \"Chatbot Responses\": [\"studying\", \"studied\", \"study\", \"studies\"],\n",
    "    \"Spam Detection\": [\"buying\", \"bought\", \"buys\", \"buy\"],\n",
    "    \"Resume Screening\": [\"developed\", \"developing\", \"developer\", \"development\"]\n",
    "}\n",
    "\n",
    "# Apply stemming and print results\n",
    "for use_case, words in examples.items():\n",
    "    lemmatizer_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    print(f\"\\n🔹 {use_case}:\\n   Original: {words}\\n   Stemmed:  {lemmatizer_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b37ca6-e4b6-4a6a-ba11-486fd11eabe0",
   "metadata": {},
   "source": [
    "# **Why Do We Use Stemming If Lemmatization Is Better?**\n",
    "\n",
    "Although lemmatization is more accurate, stemming is still used in many NLP applications because of its **speed and simplicity**. Here’s why:\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Stemming is Faster**\n",
    "- Stemming just **chops off suffixes** (e.g., `\"running\"` → `\"run\"`).\n",
    "- Lemmatization **uses a dictionary** to find the correct word, making it **slower**.\n",
    "- In large datasets (e.g., millions of documents), **stemming is preferred** for quick text processing.\n",
    "\n",
    "✅ **Example:**\n",
    "- **Stemming:** `\"computing\"` → `\"comput\"` (Fast, but not a real word)\n",
    "- **Lemmatization:** `\"computing\"` → `\"compute\"` (Correct but slower)\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Stemming is Useful in Search Engines**\n",
    "- In **Google Search, Elasticsearch, or e-commerce searches**, we need to quickly match words.\n",
    "- Users searching **\"buying\"** should still find results for **\"buy\"** even if stemming produces `\"buy\"` instead of `\"buys\"`.\n",
    "\n",
    "✅ **Example:**\n",
    "- **User searches** `\"buying laptop\"`\n",
    "- **Stemming reduces** `\"buying\"` → `\"buy\"`, matching **\"buy laptop\"** results faster.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. When High Accuracy is Not Needed**\n",
    "- If **perfect word accuracy is not required**, stemming is **good enough**.\n",
    "- Example: **Spam detection, keyword matching, sentiment analysis**.\n",
    "\n",
    "✅ **Example:**\n",
    "- **Stemming:** `\"happily\"` → `\"happili\"`\n",
    "- **Lemmatization:** `\"happily\"` → `\"happy\"`\n",
    "- In sentiment analysis, **both convey happiness**, so stemming works fine.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Stemming Uses Less Memory**\n",
    "- Since stemming is **rule-based and doesn’t use dictionaries**, it **consumes less memory**.\n",
    "- Useful in **low-power applications** (e.g., mobile NLP apps, embedded AI).\n",
    "\n",
    "---\n",
    "\n",
    "## **When to Use What?**\n",
    "\n",
    "| **Factor**                | **Stemming**  | **Lemmatization**  |\n",
    "|---------------------------|--------------|--------------------|\n",
    "| **Speed**                 | ✅ Fast      | ❌ Slower         |\n",
    "| **Accuracy**              | ❌ Less accurate | ✅ More accurate |\n",
    "| **Computational Cost**    | ✅ Low       | ❌ High           |\n",
    "| **Real-World Use**        | Search engines, quick text analysis | Chatbots, machine learning, deep NLP |\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusion:**\n",
    "✔ **Use Stemming** when speed is more important than accuracy (e.g., search engines, quick filtering).  \n",
    "✔ **Use Lemmatization** when meaning is important (e.g., chatbots, NLP models, grammar-based applications).\n",
    "\n",
    "--- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e05df3-eb60-49db-92f5-1bdc1ff4e474",
   "metadata": {},
   "source": [
    "# **TF-IDF (Term Frequency-Inverse Document Frequency)**\n",
    "\n",
    "## 📌 **What is TF-IDF?  \n",
    "TF-IDF (**Term Frequency-Inverse Document Frequency**) is a numerical statistic used in **Natural Language Processing (NLP)** to measure the importance of words in a document relative to a collection of documents (**corpus**).  \n",
    "\n",
    "It helps **identify important words** while **ignoring common words** that appear frequently but don't add much meaning (e.g., \"the\", \"is\", \"in\").  \n",
    "\n",
    "---\n",
    "\n",
    "## **1. Understanding TF (Term Frequency)**\n",
    "**TF (Term Frequency)** measures **how often a word appears** in a document.  \n",
    "The formula is:\n",
    "\n",
    "\n",
    "### TF = Number of times the word appears in a document\\Total words in the document\n",
    "\n",
    "✅ **Example:**  \n",
    "Document: `\"Machine learning is amazing. Learning is fun.\"`  \n",
    "- TF for `\"learning\"` = **2/7** = **0.2857**  \n",
    "- TF for `\"amazing\"` = **1/7** = **0.1429**  \n",
    "\n",
    "**TF alone is not enough** because common words like \"is\" and \"the\" will have high frequency but are not important.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Understanding IDF (Inverse Document Frequency)**\n",
    "**IDF (Inverse Document Frequency)** gives **less importance to common words** and **more importance to rare words** across all documents.  \n",
    "\n",
    "The formula is:\n",
    "\n",
    "\n",
    "# IDF = log ( Total number of documents / Number of documents containing the word )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "✅ **Example:**  \n",
    "# **IDF Calculation with Numbers**\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Example:**\n",
    "We have **3 documents**:\n",
    "\n",
    "1️⃣ **\"Machine learning is powerful\"**  \n",
    "2️⃣ **\"Deep learning improves AI\"**  \n",
    "3️⃣ **\"Learning algorithms are useful\"**  \n",
    "\n",
    "---\n",
    "\n",
    "### **Step 1: Count Word Occurrences Across Documents**\n",
    "| **Word**       | **Appears in how many documents?** |\n",
    "|---------------|---------------------------------|\n",
    "| **learning**   | 3 |\n",
    "| **powerful**   | 1 |\n",
    "| **deep**       | 1 |\n",
    "| **improves**   | 1 |\n",
    "| **AI**         | 1 |\n",
    "| **algorithms** | 1 |\n",
    "| **useful**     | 1 |\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2: Apply the IDF Formula**\n",
    "The formula for **Inverse Document Frequency (IDF)** is:\n",
    "\n",
    "IDF = log ( Total number of documents / Number of documents containing the word )\n",
    "\n",
    "---\n",
    "\n",
    "### **IDF for \"learning\"**\n",
    "IDF = log ( 3 / 3 ) = log(1) = **0**  \n",
    "🔹 \"learning\" appears in all 3 documents → **Low IDF (not important).**\n",
    "\n",
    "---\n",
    "\n",
    "### **IDF for \"powerful\"**\n",
    "IDF = log ( 3 / 1 ) = log(3) = **0.4771**  \n",
    "🔹 \"powerful\" appears in only 1 document → **High IDF (important word).**\n",
    "\n",
    "---\n",
    "\n",
    "### **IDF for \"AI\"**\n",
    "IDF = log ( 3 / 1 ) = log(3) = **0.4771**  \n",
    "🔹 \"AI\" is also rare, so it has a **high IDF**.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Calculating TF-IDF Score**\n",
    "\n",
    "The **TF-IDF score** is calculated using the formula:\n",
    "\n",
    "\\[\n",
    "TF-IDF = TF \\times IDF\n",
    "\\]\n",
    "\n",
    "### ✅ **Example:**\n",
    "If **TF = 0.2857** and **IDF = 0.4771**, then:\n",
    "\n",
    "\\[\n",
    "TF-IDF = 0.2857 \\times 0.4771 = 0.1363\n",
    "\\]\n",
    "\n",
    "### 🔹 **Interpretation:**  \n",
    "- A **higher TF-IDF score** means the word is **more important** in that document.  \n",
    "- A **lower TF-IDF score** means the word is **less significant** and might appear frequently in other documents.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Why is TF-IDF Important?**\n",
    "- **Improves text search (Google, Elasticsearch)**\n",
    "- **Used in chatbots and recommendation systems**\n",
    "- **Important for document classification**\n",
    "- **Removes unimportant words while keeping key terms**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b982fbc6-5376-48e9-89d4-414d3834beb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ai</th>\n",
       "      <th>algorithms</th>\n",
       "      <th>are</th>\n",
       "      <th>deep</th>\n",
       "      <th>improves</th>\n",
       "      <th>is</th>\n",
       "      <th>learning</th>\n",
       "      <th>machine</th>\n",
       "      <th>powerful</th>\n",
       "      <th>useful</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.546454</td>\n",
       "      <td>0.322745</td>\n",
       "      <td>0.546454</td>\n",
       "      <td>0.546454</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.546454</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.546454</td>\n",
       "      <td>0.546454</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.322745</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.546454</td>\n",
       "      <td>0.546454</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.322745</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.546454</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ai  algorithms       are      deep  improves        is  learning  \\\n",
       "0  0.000000    0.000000  0.000000  0.000000  0.000000  0.546454  0.322745   \n",
       "1  0.546454    0.000000  0.000000  0.546454  0.546454  0.000000  0.322745   \n",
       "2  0.000000    0.546454  0.546454  0.000000  0.000000  0.000000  0.322745   \n",
       "\n",
       "    machine  powerful    useful  \n",
       "0  0.546454  0.546454  0.000000  \n",
       "1  0.000000  0.000000  0.000000  \n",
       "2  0.000000  0.000000  0.546454  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Define the documents\n",
    "documents = [\n",
    "    \"Machine learning is powerful\",\n",
    "    \"Deep learning improves AI\",\n",
    "    \"Learning algorithms are useful\"\n",
    "]\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer\n",
    "\n",
    "# Fit and transform the documents into a TF-IDF matrix\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get feature names (words) from the vectorizer\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the TF-IDF matrix into a DataFrame\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "df_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5611ad04-4e82-4ed8-9420-9223bd67d992",
   "metadata": {},
   "source": [
    "# Example resume 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc195bea-4e71-4a4c-9f1d-96ab433875fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_text = \"\"\"\n",
    "John Doe\n",
    "Email: johndoe@example.com | LinkedIn: linkedin.com/in/johndoe | GitHub: github.com/johndoe\n",
    "\n",
    "🔹 **Professional Summary:**\n",
    "Data Scientist with **5+ years of experience** in machine learning, deep learning, and AI-driven solutions. Proven ability to analyze large datasets, develop predictive models, and deploy scalable AI applications. Passionate about leveraging data science to drive business insights and automation.\n",
    "\n",
    "🔹 **Technical Skills:**\n",
    "- Programming: Python, R, SQL, Scala\n",
    "- Machine Learning: Scikit-Learn, XGBoost, LightGBM\n",
    "- Deep Learning: TensorFlow, PyTorch, Keras\n",
    "- Data Engineering: Spark, Hadoop, Airflow, ETL Pipelines\n",
    "- Visualization: Matplotlib, Seaborn, Power BI, Tableau\n",
    "- Cloud Platforms: AWS (S3, Lambda, SageMaker), Azure, Google Cloud (BigQuery)\n",
    "\n",
    "🔹 **Work Experience:**\n",
    "**Senior Data Scientist** | ABC Tech Solutions | Jan 2020 – Present  \n",
    "- Developed an **AI-driven fraud detection system** reducing fraudulent transactions by 35%.\n",
    "- Designed and deployed a **real-time recommendation engine** using collaborative filtering, improving customer retention by 25%.\n",
    "- Spearheaded an **automated NLP pipeline** for sentiment analysis on 100,000+ customer reviews.\n",
    "- Built and deployed **ML models for demand forecasting**, reducing inventory costs by 20%.\n",
    "\n",
    "**Data Scientist** | XYZ Analytics | July 2017 – Dec 2019  \n",
    "- Created **predictive risk models** for financial services, reducing loan default rates by 18%.\n",
    "- Led a **big data analytics initiative**, processing 5TB+ data weekly using Spark.\n",
    "- Improved **email marketing campaign targeting**, increasing conversion rates by 22%.\n",
    "- Conducted **A/B testing** and customer segmentation analysis for data-driven decision-making.\n",
    "\n",
    "🔹 **Education:**\n",
    "Master’s in Data Science | Stanford University | 2017  \n",
    "Bachelor’s in Computer Science | University of California, Berkeley | 2015  \n",
    "\n",
    "🔹 **Certifications:**\n",
    "- Google Professional Data Engineer  \n",
    "- AWS Certified Machine Learning – Specialty  \n",
    "- TensorFlow Developer Certification  \n",
    "\n",
    "🔹 **Projects:**\n",
    "- Developed **anomaly detection models** for network security using Autoencoders.\n",
    "- Built an **AI chatbot** for automated customer support using NLP techniques.\n",
    "- Designed a **real-time dashboard** for sales forecasting using Power BI.\n",
    "\n",
    "🔹 **Publications & Research:**\n",
    "- \"Enhancing Fraud Detection with Machine Learning\" – Published in IEEE  \n",
    "- Speaker at **PyCon 2022** on Explainable AI  \n",
    "\n",
    "🔹 **Soft Skills:**\n",
    "- Strong problem-solving and analytical skills  \n",
    "- Effective communication and stakeholder collaboration  \n",
    "- Agile and cross-functional team leadership  \n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b155f715-60e5-4b6d-8d9f-5415837afed0",
   "metadata": {},
   "source": [
    "# Job description Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7528e86c-f602-407a-a73c-96ef060955e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_description_1 = \"\"\"\n",
    "Job Title: Senior Data Scientist  \n",
    "Location: San Francisco, CA | Remote Option Available  \n",
    "\n",
    "Job Overview:  \n",
    "We are seeking an experienced Senior Data Scientist to develop machine learning models, deep learning solutions, and AI-driven insights. You will work with large-scale datasets, build predictive analytics solutions, and deploy AI models in production environments.  \n",
    "\n",
    "Key Responsibilities:  \n",
    "- Develop and deploy machine learning and deep learning models using Scikit-Learn, TensorFlow, and PyTorch.  \n",
    "- Build real-time recommendation engines and fraud detection systems to improve business performance.  \n",
    "- Work with big data technologies (Spark, Hadoop) for data preprocessing and model training.  \n",
    "- Create data pipelines and manage ETL processes for structured and unstructured data.  \n",
    "- Design AI-powered NLP solutions for sentiment analysis and chatbots.  \n",
    "- Collaborate with engineering teams to deploy ML models on AWS, GCP, or Azure.  \n",
    "- Conduct A/B testing, customer segmentation, and data visualization using Power BI or Tableau.  \n",
    "\n",
    "Required Qualifications:  \n",
    "- 5+ years of experience in Data Science and AI model deployment.  \n",
    "- Expertise in Python, SQL, and Scala for data analysis and modeling.  \n",
    "- Strong background in Machine Learning, Deep Learning, and NLP.  \n",
    "- Hands-on experience with Big Data (Spark, Hadoop, Airflow).  \n",
    "- Proficiency in TensorFlow, PyTorch, Keras, and Scikit-Learn.  \n",
    "- Experience working with AWS SageMaker, GCP BigQuery, and Azure ML.  \n",
    "- Excellent problem-solving skills and ability to work in agile teams.  \n",
    "\n",
    "Preferred Qualifications:  \n",
    "- Google or AWS Certified Machine Learning Engineer.  \n",
    "- Experience with real-time analytics and AI product deployment.  \n",
    "\n",
    "Salary: 15,00,000 - 18,00,000 + Benefits  \n",
    "Job Type: Full-Time | Remote / Hybrid Option  \n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f1a4399-b92a-4e89-ad1a-9f156b644e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_description_2 = \"\"\"\n",
    "Job Title: Data Engineer / AI Specialist  \n",
    "Location: New York, NY  \n",
    "\n",
    "Job Overview:  \n",
    "We are looking for a Data Engineer with AI experience to build and optimize data pipelines, work on machine learning solutions, and contribute to AI-based projects.  \n",
    "\n",
    "Key Responsibilities:  \n",
    "- Build ETL pipelines and manage big data platforms (Hadoop, Spark).  \n",
    "- Support ML model deployment and collaborate with Data Scientists.  \n",
    "- Develop automated reporting dashboards using Power BI and Tableau.  \n",
    "- Work on real-time data ingestion and transformation processes.  \n",
    "- Assist in training and fine-tuning ML models in cloud environments.  \n",
    "- Implement SQL-based data storage solutions for structured datasets.  \n",
    "\n",
    "Required Qualifications:  \n",
    "- 3+ years of experience in Data Engineering or Machine Learning.  \n",
    "- Strong experience with SQL, Python, and Big Data Tools (Spark, Hadoop, Kafka).  \n",
    "- Familiarity with Cloud Computing (AWS, GCP, or Azure).  \n",
    "- Understanding of AI workflows and data-driven insights.  \n",
    "\n",
    "Preferred Qualifications:  \n",
    "- Experience working with Scikit-Learn, TensorFlow, or PyTorch.  \n",
    "- Background in recommendation systems or AI analytics.  \n",
    "\n",
    "Salary: 12,00,000 - 14,00,000 + Bonus  \n",
    "Job Type: Full-Time | On-site  \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "36821ac8-7657-4a48-b61f-af768a21ac75",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_description_3 = \"\"\"\n",
    "Job Title: Business Intelligence Analyst  \n",
    "Location: Chicago, IL  \n",
    "\n",
    "Job Overview:  \n",
    "We are hiring a Business Intelligence (BI) Analyst to develop reports, analyze market trends, and create dashboards for executive decision-making.  \n",
    "\n",
    "Key Responsibilities:  \n",
    "- Design business intelligence dashboards in Power BI and Tableau.  \n",
    "- Perform market research and business analytics to drive strategic decisions.  \n",
    "- Work with Excel, SQL, and statistical models for financial forecasting.  \n",
    "- Collaborate with marketing and sales teams to identify customer insights.  \n",
    "- Develop monthly reports and KPI tracking for management.  \n",
    "\n",
    "Required Qualifications:  \n",
    "- 2+ years of experience in Business Intelligence or Market Analytics.  \n",
    "- Proficiency in SQL, Excel, and BI Tools (Power BI, Tableau).  \n",
    "- Strong background in data visualization and business reporting.  \n",
    "\n",
    "Preferred Qualifications:  \n",
    "- Experience with CRM analytics and sales forecasting.  \n",
    "- Basic knowledge of Python or R for data modeling.  \n",
    "\n",
    "Salary: 80,00,000 - 10,00,000  \n",
    "Job Type: Full-Time | Hybrid  \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83e65e9f-e977-406a-a5e2-5cc91a0b0de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Text:\n",
      " \n",
      "John Doe\n",
      "Email: johndoe@example.com | LinkedIn: linkedin.com/in/johndoe | GitHub: github.com/johndoe\n",
      "\n",
      "🔹 **Professional Summary:**\n",
      "Data Scientist with **5+ years of experience** in machine learning, deep learning, and AI-driven solutions. Proven ability to analyze large datasets, develop predictive models, and deploy scalable AI applications. Passionate about leveraging data science to drive business insights and automation.\n",
      "\n",
      "🔹 **Technical Skills:**\n",
      "- Programming: Python, R, SQL, Scala\n",
      "- Machine Learning: Scikit-Learn, XGBoost, LightGBM\n",
      "- Deep Learning: TensorFlow, PyTorch, Keras\n",
      "- Data Engineering: Spark, Hadoop, Airflow, ETL Pipelines\n",
      "- Visualization: Matplotlib, Seaborn, Power BI, Tableau\n",
      "- Cloud Platforms: AWS (S3, Lambda, SageMaker), Azure, Google Cloud (BigQuery)\n",
      "\n",
      "🔹 **Work Experience:**\n",
      "**Senior Data Scientist** | ABC Tech Solutions | Jan 2020 – Present  \n",
      "- Developed an **AI-driven fraud detection system** reducing fraudulent transactions by 35%.\n",
      "- Designed and deployed a **real-time recommendation engine** using collaborative filtering, improving customer retention by 25%.\n",
      "- Spearheaded an **automated NLP pipeline** for sentiment analysis on 100,000+ customer reviews.\n",
      "- Built and deployed **ML models for demand forecasting**, reducing inventory costs by 20%.\n",
      "\n",
      "**Data Scientist** | XYZ Analytics | July 2017 – Dec 2019  \n",
      "- Created **predictive risk models** for financial services, reducing loan default rates by 18%.\n",
      "- Led a **big data analytics initiative**, processing 5TB+ data weekly using Spark.\n",
      "- Improved **email marketing campaign targeting**, increasing conversion rates by 22%.\n",
      "- Conducted **A/B testing** and customer segmentation analysis for data-driven decision-making.\n",
      "\n",
      "🔹 **Education:**\n",
      "Master’s in Data Science | Stanford University | 2017  \n",
      "Bachelor’s in Computer Science | University of California, Berkeley | 2015  \n",
      "\n",
      "🔹 **Certifications:**\n",
      "- Google Professional Data Engineer  \n",
      "- AWS Certified Machine Learning – Specialty  \n",
      "- TensorFlow Developer Certification  \n",
      "\n",
      "🔹 **Projects:**\n",
      "- Developed **anomaly detection models** for network security using Autoencoders.\n",
      "- Built an **AI chatbot** for automated customer support using NLP techniques.\n",
      "- Designed a **real-time dashboard** for sales forecasting using Power BI.\n",
      "\n",
      "🔹 **Publications & Research:**\n",
      "- \"Enhancing Fraud Detection with Machine Learning\" – Published in IEEE  \n",
      "- Speaker at **PyCon 2022** on Explainable AI  \n",
      "\n",
      "🔹 **Soft Skills:**\n",
      "- Strong problem-solving and analytical skills  \n",
      "- Effective communication and stakeholder collaboration  \n",
      "- Agile and cross-functional team leadership  \n",
      "\n",
      "\n",
      "\n",
      "Tokenization:\n",
      " ['john', 'doe', 'email', 'johndoe', 'example', 'com', 'linkedin', 'linkedin', 'com', 'in', 'johndoe', 'github', 'github', 'com', 'johndoe', 'professional', 'summary', 'data', 'scientist', 'with', '5', 'years', 'of', 'experience', 'in', 'machine', 'learning', 'deep', 'learning', 'and', 'ai', 'driven', 'solutions', 'proven', 'ability', 'to', 'analyze', 'large', 'datasets', 'develop', 'predictive', 'models', 'and', 'deploy', 'scalable', 'ai', 'applications', 'passionate', 'about', 'leveraging', 'data', 'science', 'to', 'drive', 'business', 'insights', 'and', 'automation', 'technical', 'skills', 'programming', 'python', 'r', 'sql', 'scala', 'machine', 'learning', 'scikit', 'learn', 'xgboost', 'lightgbm', 'deep', 'learning', 'tensorflow', 'pytorch', 'keras', 'data', 'engineering', 'spark', 'hadoop', 'airflow', 'etl', 'pipelines', 'visualization', 'matplotlib', 'seaborn', 'power', 'bi', 'tableau', 'cloud', 'platforms', 'aws', 's3', 'lambda', 'sagemaker', 'azure', 'google', 'cloud', 'bigquery', 'work', 'experience', 'senior', 'data', 'scientist', 'abc', 'tech', 'solutions', 'jan', '2020', 'present', 'developed', 'an', 'ai', 'driven', 'fraud', 'detection', 'system', 'reducing', 'fraudulent', 'transactions', 'by', '35', 'designed', 'and', 'deployed', 'a', 'real', 'time', 'recommendation', 'engine', 'using', 'collaborative', 'filtering', 'improving', 'customer', 'retention', 'by', '25', 'spearheaded', 'an', 'automated', 'nlp', 'pipeline', 'for', 'sentiment', 'analysis', 'on', '100', '000', 'customer', 'reviews', 'built', 'and', 'deployed', 'ml', 'models', 'for', 'demand', 'forecasting', 'reducing', 'inventory', 'costs', 'by', '20', 'data', 'scientist', 'xyz', 'analytics', 'july', '2017', 'dec', '2019', 'created', 'predictive', 'risk', 'models', 'for', 'financial', 'services', 'reducing', 'loan', 'default', 'rates', 'by', '18', 'led', 'a', 'big', 'data', 'analytics', 'initiative', 'processing', '5tb', 'data', 'weekly', 'using', 'spark', 'improved', 'email', 'marketing', 'campaign', 'targeting', 'increasing', 'conversion', 'rates', 'by', '22', 'conducted', 'a', 'b', 'testing', 'and', 'customer', 'segmentation', 'analysis', 'for', 'data', 'driven', 'decision', 'making', 'education', 'master', 's', 'in', 'data', 'science', 'stanford', 'university', '2017', 'bachelor', 's', 'in', 'computer', 'science', 'university', 'of', 'california', 'berkeley', '2015', 'certifications', 'google', 'professional', 'data', 'engineer', 'aws', 'certified', 'machine', 'learning', 'specialty', 'tensorflow', 'developer', 'certification', 'projects', 'developed', 'anomaly', 'detection', 'models', 'for', 'network', 'security', 'using', 'autoencoders', 'built', 'an', 'ai', 'chatbot', 'for', 'automated', 'customer', 'support', 'using', 'nlp', 'techniques', 'designed', 'a', 'real', 'time', 'dashboard', 'for', 'sales', 'forecasting', 'using', 'power', 'bi', 'publications', 'research', 'enhancing', 'fraud', 'detection', 'with', 'machine', 'learning', 'published', 'in', 'ieee', 'speaker', 'at', 'pycon', '2022', 'on', 'explainable', 'ai', 'soft', 'skills', 'strong', 'problem', 'solving', 'and', 'analytical', 'skills', 'effective', 'communication', 'and', 'stakeholder', 'collaboration', 'agile', 'and', 'cross', 'functional', 'team', 'leadership']\n",
      "\n",
      "After Stopword Removal:\n",
      " ['john', 'doe', 'email', 'johndoe', 'example', 'com', 'linkedin', 'linkedin', 'com', 'johndoe', 'github', 'github', 'com', 'johndoe', 'professional', 'summary', 'data', 'scientist', '5', 'years', 'experience', 'machine', 'learning', 'deep', 'learning', 'ai', 'driven', 'solutions', 'proven', 'ability', 'analyze', 'large', 'datasets', 'develop', 'predictive', 'models', 'deploy', 'scalable', 'ai', 'applications', 'passionate', 'leveraging', 'data', 'science', 'drive', 'business', 'insights', 'automation', 'technical', 'skills', 'programming', 'python', 'r', 'sql', 'scala', 'machine', 'learning', 'scikit', 'learn', 'xgboost', 'lightgbm', 'deep', 'learning', 'tensorflow', 'pytorch', 'keras', 'data', 'engineering', 'spark', 'hadoop', 'airflow', 'etl', 'pipelines', 'visualization', 'matplotlib', 'seaborn', 'power', 'bi', 'tableau', 'cloud', 'platforms', 'aws', 's3', 'lambda', 'sagemaker', 'azure', 'google', 'cloud', 'bigquery', 'work', 'experience', 'senior', 'data', 'scientist', 'abc', 'tech', 'solutions', 'jan', '2020', 'present', 'developed', 'ai', 'driven', 'fraud', 'detection', 'system', 'reducing', 'fraudulent', 'transactions', '35', 'designed', 'deployed', 'real', 'time', 'recommendation', 'engine', 'using', 'collaborative', 'filtering', 'improving', 'customer', 'retention', '25', 'spearheaded', 'automated', 'nlp', 'pipeline', 'sentiment', 'analysis', '100', '000', 'customer', 'reviews', 'built', 'deployed', 'ml', 'models', 'demand', 'forecasting', 'reducing', 'inventory', 'costs', '20', 'data', 'scientist', 'xyz', 'analytics', 'july', '2017', 'dec', '2019', 'created', 'predictive', 'risk', 'models', 'financial', 'services', 'reducing', 'loan', 'default', 'rates', '18', 'led', 'big', 'data', 'analytics', 'initiative', 'processing', '5tb', 'data', 'weekly', 'using', 'spark', 'improved', 'email', 'marketing', 'campaign', 'targeting', 'increasing', 'conversion', 'rates', '22', 'conducted', 'b', 'testing', 'customer', 'segmentation', 'analysis', 'data', 'driven', 'decision', 'making', 'education', 'master', 'data', 'science', 'stanford', 'university', '2017', 'bachelor', 'computer', 'science', 'university', 'california', 'berkeley', '2015', 'certifications', 'google', 'professional', 'data', 'engineer', 'aws', 'certified', 'machine', 'learning', 'specialty', 'tensorflow', 'developer', 'certification', 'projects', 'developed', 'anomaly', 'detection', 'models', 'network', 'security', 'using', 'autoencoders', 'built', 'ai', 'chatbot', 'automated', 'customer', 'support', 'using', 'nlp', 'techniques', 'designed', 'real', 'time', 'dashboard', 'sales', 'forecasting', 'using', 'power', 'bi', 'publications', 'research', 'enhancing', 'fraud', 'detection', 'machine', 'learning', 'published', 'ieee', 'speaker', 'pycon', '2022', 'explainable', 'ai', 'soft', 'skills', 'strong', 'problem', 'solving', 'analytical', 'skills', 'effective', 'communication', 'stakeholder', 'collaboration', 'agile', 'cross', 'functional', 'team', 'leadership']\n",
      "\n",
      "After Stemming:\n",
      " ['john', 'doe', 'email', 'johndo', 'exampl', 'com', 'linkedin', 'linkedin', 'com', 'johndo', 'github', 'github', 'com', 'johndo', 'profession', 'summari', 'data', 'scientist', '5', 'year', 'experi', 'machin', 'learn', 'deep', 'learn', 'ai', 'driven', 'solut', 'proven', 'abil', 'analyz', 'larg', 'dataset', 'develop', 'predict', 'model', 'deploy', 'scalabl', 'ai', 'applic', 'passion', 'leverag', 'data', 'scienc', 'drive', 'busi', 'insight', 'autom', 'technic', 'skill', 'program', 'python', 'r', 'sql', 'scala', 'machin', 'learn', 'scikit', 'learn', 'xgboost', 'lightgbm', 'deep', 'learn', 'tensorflow', 'pytorch', 'kera', 'data', 'engin', 'spark', 'hadoop', 'airflow', 'etl', 'pipelin', 'visual', 'matplotlib', 'seaborn', 'power', 'bi', 'tableau', 'cloud', 'platform', 'aw', 's3', 'lambda', 'sagemak', 'azur', 'googl', 'cloud', 'bigqueri', 'work', 'experi', 'senior', 'data', 'scientist', 'abc', 'tech', 'solut', 'jan', '2020', 'present', 'develop', 'ai', 'driven', 'fraud', 'detect', 'system', 'reduc', 'fraudul', 'transact', '35', 'design', 'deploy', 'real', 'time', 'recommend', 'engin', 'use', 'collabor', 'filter', 'improv', 'custom', 'retent', '25', 'spearhead', 'autom', 'nlp', 'pipelin', 'sentiment', 'analysi', '100', '000', 'custom', 'review', 'built', 'deploy', 'ml', 'model', 'demand', 'forecast', 'reduc', 'inventori', 'cost', '20', 'data', 'scientist', 'xyz', 'analyt', 'juli', '2017', 'dec', '2019', 'creat', 'predict', 'risk', 'model', 'financi', 'servic', 'reduc', 'loan', 'default', 'rate', '18', 'led', 'big', 'data', 'analyt', 'initi', 'process', '5tb', 'data', 'weekli', 'use', 'spark', 'improv', 'email', 'market', 'campaign', 'target', 'increas', 'convers', 'rate', '22', 'conduct', 'b', 'test', 'custom', 'segment', 'analysi', 'data', 'driven', 'decis', 'make', 'educ', 'master', 'data', 'scienc', 'stanford', 'univers', '2017', 'bachelor', 'comput', 'scienc', 'univers', 'california', 'berkeley', '2015', 'certif', 'googl', 'profession', 'data', 'engin', 'aw', 'certifi', 'machin', 'learn', 'specialti', 'tensorflow', 'develop', 'certif', 'project', 'develop', 'anomali', 'detect', 'model', 'network', 'secur', 'use', 'autoencod', 'built', 'ai', 'chatbot', 'autom', 'custom', 'support', 'use', 'nlp', 'techniqu', 'design', 'real', 'time', 'dashboard', 'sale', 'forecast', 'use', 'power', 'bi', 'public', 'research', 'enhanc', 'fraud', 'detect', 'machin', 'learn', 'publish', 'ieee', 'speaker', 'pycon', '2022', 'explain', 'ai', 'soft', 'skill', 'strong', 'problem', 'solv', 'analyt', 'skill', 'effect', 'commun', 'stakehold', 'collabor', 'agil', 'cross', 'function', 'team', 'leadership']\n",
      "\n",
      "After Lemmatization:\n",
      " ['john', 'doe', 'email', 'johndo', 'exampl', 'com', 'linkedin', 'linkedin', 'com', 'johndo', 'github', 'github', 'com', 'johndo', 'profession', 'summari', 'data', 'scientist', '5', 'year', 'experi', 'machin', 'learn', 'deep', 'learn', 'ai', 'drive', 'solut', 'prove', 'abil', 'analyz', 'larg', 'dataset', 'develop', 'predict', 'model', 'deploy', 'scalabl', 'ai', 'applic', 'passion', 'leverag', 'data', 'scienc', 'drive', 'busi', 'insight', 'autom', 'technic', 'skill', 'program', 'python', 'r', 'sql', 'scala', 'machin', 'learn', 'scikit', 'learn', 'xgboost', 'lightgbm', 'deep', 'learn', 'tensorflow', 'pytorch', 'kera', 'data', 'engin', 'spark', 'hadoop', 'airflow', 'etl', 'pipelin', 'visual', 'matplotlib', 'seaborn', 'power', 'bi', 'tableau', 'cloud', 'platform', 'aw', 's3', 'lambda', 'sagemak', 'azur', 'googl', 'cloud', 'bigqueri', 'work', 'experi', 'senior', 'data', 'scientist', 'abc', 'tech', 'solut', 'jan', '2020', 'present', 'develop', 'ai', 'drive', 'fraud', 'detect', 'system', 'reduc', 'fraudul', 'transact', '35', 'design', 'deploy', 'real', 'time', 'recommend', 'engin', 'use', 'collabor', 'filter', 'improv', 'custom', 'retent', '25', 'spearhead', 'autom', 'nlp', 'pipelin', 'sentiment', 'analysi', '100', '000', 'custom', 'review', 'build', 'deploy', 'ml', 'model', 'demand', 'forecast', 'reduc', 'inventori', 'cost', '20', 'data', 'scientist', 'xyz', 'analyt', 'juli', '2017', 'dec', '2019', 'creat', 'predict', 'risk', 'model', 'financi', 'servic', 'reduc', 'loan', 'default', 'rate', '18', 'lead', 'big', 'data', 'analyt', 'initi', 'process', '5tb', 'data', 'weekli', 'use', 'spark', 'improv', 'email', 'market', 'campaign', 'target', 'increas', 'convers', 'rate', '22', 'conduct', 'b', 'test', 'custom', 'segment', 'analysi', 'data', 'drive', 'decis', 'make', 'educ', 'master', 'data', 'scienc', 'stanford', 'univers', '2017', 'bachelor', 'comput', 'scienc', 'univers', 'california', 'berkeley', '2015', 'certif', 'googl', 'profession', 'data', 'engin', 'aw', 'certifi', 'machin', 'learn', 'specialti', 'tensorflow', 'develop', 'certif', 'project', 'develop', 'anomali', 'detect', 'model', 'network', 'secur', 'use', 'autoencod', 'build', 'ai', 'chatbot', 'autom', 'custom', 'support', 'use', 'nlp', 'techniqu', 'design', 'real', 'time', 'dashboard', 'sale', 'forecast', 'use', 'power', 'bi', 'public', 'research', 'enhanc', 'fraud', 'detect', 'machin', 'learn', 'publish', 'ieee', 'speaker', 'pycon', '2022', 'explain', 'ai', 'soft', 'skill', 'strong', 'problem', 'solv', 'analyt', 'skill', 'effect', 'commun', 'stakehold', 'collabor', 'agil', 'cross', 'function', 'team', 'leadership']\n",
      "\n",
      "Original Text:\n",
      " \n",
      "Job Title: Senior Data Scientist  \n",
      "Location: San Francisco, CA | Remote Option Available  \n",
      "\n",
      "Job Overview:  \n",
      "We are seeking an experienced Senior Data Scientist to develop machine learning models, deep learning solutions, and AI-driven insights. You will work with large-scale datasets, build predictive analytics solutions, and deploy AI models in production environments.  \n",
      "\n",
      "Key Responsibilities:  \n",
      "- Develop and deploy machine learning and deep learning models using Scikit-Learn, TensorFlow, and PyTorch.  \n",
      "- Build real-time recommendation engines and fraud detection systems to improve business performance.  \n",
      "- Work with big data technologies (Spark, Hadoop) for data preprocessing and model training.  \n",
      "- Create data pipelines and manage ETL processes for structured and unstructured data.  \n",
      "- Design AI-powered NLP solutions for sentiment analysis and chatbots.  \n",
      "- Collaborate with engineering teams to deploy ML models on AWS, GCP, or Azure.  \n",
      "- Conduct A/B testing, customer segmentation, and data visualization using Power BI or Tableau.  \n",
      "\n",
      "Required Qualifications:  \n",
      "- 5+ years of experience in Data Science and AI model deployment.  \n",
      "- Expertise in Python, SQL, and Scala for data analysis and modeling.  \n",
      "- Strong background in Machine Learning, Deep Learning, and NLP.  \n",
      "- Hands-on experience with Big Data (Spark, Hadoop, Airflow).  \n",
      "- Proficiency in TensorFlow, PyTorch, Keras, and Scikit-Learn.  \n",
      "- Experience working with AWS SageMaker, GCP BigQuery, and Azure ML.  \n",
      "- Excellent problem-solving skills and ability to work in agile teams.  \n",
      "\n",
      "Preferred Qualifications:  \n",
      "- Google or AWS Certified Machine Learning Engineer.  \n",
      "- Experience with real-time analytics and AI product deployment.  \n",
      "\n",
      "Salary: 15,00,000 - 18,00,000 + Benefits  \n",
      "Job Type: Full-Time | Remote / Hybrid Option  \n",
      "\n",
      "\n",
      "Tokenization:\n",
      " ['job', 'title', 'senior', 'data', 'scientist', 'location', 'san', 'francisco', 'ca', 'remote', 'option', 'available', 'job', 'overview', 'we', 'are', 'seeking', 'an', 'experienced', 'senior', 'data', 'scientist', 'to', 'develop', 'machine', 'learning', 'models', 'deep', 'learning', 'solutions', 'and', 'ai', 'driven', 'insights', 'you', 'will', 'work', 'with', 'large', 'scale', 'datasets', 'build', 'predictive', 'analytics', 'solutions', 'and', 'deploy', 'ai', 'models', 'in', 'production', 'environments', 'key', 'responsibilities', 'develop', 'and', 'deploy', 'machine', 'learning', 'and', 'deep', 'learning', 'models', 'using', 'scikit', 'learn', 'tensorflow', 'and', 'pytorch', 'build', 'real', 'time', 'recommendation', 'engines', 'and', 'fraud', 'detection', 'systems', 'to', 'improve', 'business', 'performance', 'work', 'with', 'big', 'data', 'technologies', 'spark', 'hadoop', 'for', 'data', 'preprocessing', 'and', 'model', 'training', 'create', 'data', 'pipelines', 'and', 'manage', 'etl', 'processes', 'for', 'structured', 'and', 'unstructured', 'data', 'design', 'ai', 'powered', 'nlp', 'solutions', 'for', 'sentiment', 'analysis', 'and', 'chatbots', 'collaborate', 'with', 'engineering', 'teams', 'to', 'deploy', 'ml', 'models', 'on', 'aws', 'gcp', 'or', 'azure', 'conduct', 'a', 'b', 'testing', 'customer', 'segmentation', 'and', 'data', 'visualization', 'using', 'power', 'bi', 'or', 'tableau', 'required', 'qualifications', '5', 'years', 'of', 'experience', 'in', 'data', 'science', 'and', 'ai', 'model', 'deployment', 'expertise', 'in', 'python', 'sql', 'and', 'scala', 'for', 'data', 'analysis', 'and', 'modeling', 'strong', 'background', 'in', 'machine', 'learning', 'deep', 'learning', 'and', 'nlp', 'hands', 'on', 'experience', 'with', 'big', 'data', 'spark', 'hadoop', 'airflow', 'proficiency', 'in', 'tensorflow', 'pytorch', 'keras', 'and', 'scikit', 'learn', 'experience', 'working', 'with', 'aws', 'sagemaker', 'gcp', 'bigquery', 'and', 'azure', 'ml', 'excellent', 'problem', 'solving', 'skills', 'and', 'ability', 'to', 'work', 'in', 'agile', 'teams', 'preferred', 'qualifications', 'google', 'or', 'aws', 'certified', 'machine', 'learning', 'engineer', 'experience', 'with', 'real', 'time', 'analytics', 'and', 'ai', 'product', 'deployment', 'salary', '15', '00', '000', '18', '00', '000', 'benefits', 'job', 'type', 'full', 'time', 'remote', 'hybrid', 'option']\n",
      "\n",
      "After Stopword Removal:\n",
      " ['job', 'title', 'senior', 'data', 'scientist', 'location', 'san', 'francisco', 'ca', 'remote', 'option', 'available', 'job', 'overview', 'seeking', 'experienced', 'senior', 'data', 'scientist', 'develop', 'machine', 'learning', 'models', 'deep', 'learning', 'solutions', 'ai', 'driven', 'insights', 'work', 'large', 'scale', 'datasets', 'build', 'predictive', 'analytics', 'solutions', 'deploy', 'ai', 'models', 'production', 'environments', 'key', 'responsibilities', 'develop', 'deploy', 'machine', 'learning', 'deep', 'learning', 'models', 'using', 'scikit', 'learn', 'tensorflow', 'pytorch', 'build', 'real', 'time', 'recommendation', 'engines', 'fraud', 'detection', 'systems', 'improve', 'business', 'performance', 'work', 'big', 'data', 'technologies', 'spark', 'hadoop', 'data', 'preprocessing', 'model', 'training', 'create', 'data', 'pipelines', 'manage', 'etl', 'processes', 'structured', 'unstructured', 'data', 'design', 'ai', 'powered', 'nlp', 'solutions', 'sentiment', 'analysis', 'chatbots', 'collaborate', 'engineering', 'teams', 'deploy', 'ml', 'models', 'aws', 'gcp', 'azure', 'conduct', 'b', 'testing', 'customer', 'segmentation', 'data', 'visualization', 'using', 'power', 'bi', 'tableau', 'required', 'qualifications', '5', 'years', 'experience', 'data', 'science', 'ai', 'model', 'deployment', 'expertise', 'python', 'sql', 'scala', 'data', 'analysis', 'modeling', 'strong', 'background', 'machine', 'learning', 'deep', 'learning', 'nlp', 'hands', 'experience', 'big', 'data', 'spark', 'hadoop', 'airflow', 'proficiency', 'tensorflow', 'pytorch', 'keras', 'scikit', 'learn', 'experience', 'working', 'aws', 'sagemaker', 'gcp', 'bigquery', 'azure', 'ml', 'excellent', 'problem', 'solving', 'skills', 'ability', 'work', 'agile', 'teams', 'preferred', 'qualifications', 'google', 'aws', 'certified', 'machine', 'learning', 'engineer', 'experience', 'real', 'time', 'analytics', 'ai', 'product', 'deployment', 'salary', '15', '00', '000', '18', '00', '000', 'benefits', 'job', 'type', 'full', 'time', 'remote', 'hybrid', 'option']\n",
      "\n",
      "After Stemming:\n",
      " ['job', 'titl', 'senior', 'data', 'scientist', 'locat', 'san', 'francisco', 'ca', 'remot', 'option', 'avail', 'job', 'overview', 'seek', 'experienc', 'senior', 'data', 'scientist', 'develop', 'machin', 'learn', 'model', 'deep', 'learn', 'solut', 'ai', 'driven', 'insight', 'work', 'larg', 'scale', 'dataset', 'build', 'predict', 'analyt', 'solut', 'deploy', 'ai', 'model', 'product', 'environ', 'key', 'respons', 'develop', 'deploy', 'machin', 'learn', 'deep', 'learn', 'model', 'use', 'scikit', 'learn', 'tensorflow', 'pytorch', 'build', 'real', 'time', 'recommend', 'engin', 'fraud', 'detect', 'system', 'improv', 'busi', 'perform', 'work', 'big', 'data', 'technolog', 'spark', 'hadoop', 'data', 'preprocess', 'model', 'train', 'creat', 'data', 'pipelin', 'manag', 'etl', 'process', 'structur', 'unstructur', 'data', 'design', 'ai', 'power', 'nlp', 'solut', 'sentiment', 'analysi', 'chatbot', 'collabor', 'engin', 'team', 'deploy', 'ml', 'model', 'aw', 'gcp', 'azur', 'conduct', 'b', 'test', 'custom', 'segment', 'data', 'visual', 'use', 'power', 'bi', 'tableau', 'requir', 'qualif', '5', 'year', 'experi', 'data', 'scienc', 'ai', 'model', 'deploy', 'expertis', 'python', 'sql', 'scala', 'data', 'analysi', 'model', 'strong', 'background', 'machin', 'learn', 'deep', 'learn', 'nlp', 'hand', 'experi', 'big', 'data', 'spark', 'hadoop', 'airflow', 'profici', 'tensorflow', 'pytorch', 'kera', 'scikit', 'learn', 'experi', 'work', 'aw', 'sagemak', 'gcp', 'bigqueri', 'azur', 'ml', 'excel', 'problem', 'solv', 'skill', 'abil', 'work', 'agil', 'team', 'prefer', 'qualif', 'googl', 'aw', 'certifi', 'machin', 'learn', 'engin', 'experi', 'real', 'time', 'analyt', 'ai', 'product', 'deploy', 'salari', '15', '00', '000', '18', '00', '000', 'benefit', 'job', 'type', 'full', 'time', 'remot', 'hybrid', 'option']\n",
      "\n",
      "After Lemmatization:\n",
      " ['job', 'titl', 'senior', 'data', 'scientist', 'locat', 'san', 'francisco', 'ca', 'remot', 'option', 'avail', 'job', 'overview', 'seek', 'experienc', 'senior', 'data', 'scientist', 'develop', 'machin', 'learn', 'model', 'deep', 'learn', 'solut', 'ai', 'drive', 'insight', 'work', 'larg', 'scale', 'dataset', 'build', 'predict', 'analyt', 'solut', 'deploy', 'ai', 'model', 'product', 'environ', 'key', 'respons', 'develop', 'deploy', 'machin', 'learn', 'deep', 'learn', 'model', 'use', 'scikit', 'learn', 'tensorflow', 'pytorch', 'build', 'real', 'time', 'recommend', 'engin', 'fraud', 'detect', 'system', 'improv', 'busi', 'perform', 'work', 'big', 'data', 'technolog', 'spark', 'hadoop', 'data', 'preprocess', 'model', 'train', 'creat', 'data', 'pipelin', 'manag', 'etl', 'process', 'structur', 'unstructur', 'data', 'design', 'ai', 'power', 'nlp', 'solut', 'sentiment', 'analysi', 'chatbot', 'collabor', 'engin', 'team', 'deploy', 'ml', 'model', 'aw', 'gcp', 'azur', 'conduct', 'b', 'test', 'custom', 'segment', 'data', 'visual', 'use', 'power', 'bi', 'tableau', 'requir', 'qualif', '5', 'year', 'experi', 'data', 'scienc', 'ai', 'model', 'deploy', 'expertis', 'python', 'sql', 'scala', 'data', 'analysi', 'model', 'strong', 'background', 'machin', 'learn', 'deep', 'learn', 'nlp', 'hand', 'experi', 'big', 'data', 'spark', 'hadoop', 'airflow', 'profici', 'tensorflow', 'pytorch', 'kera', 'scikit', 'learn', 'experi', 'work', 'aw', 'sagemak', 'gcp', 'bigqueri', 'azur', 'ml', 'excel', 'problem', 'solv', 'skill', 'abil', 'work', 'agil', 'team', 'prefer', 'qualif', 'googl', 'aw', 'certifi', 'machin', 'learn', 'engin', 'experi', 'real', 'time', 'analyt', 'ai', 'product', 'deploy', 'salari', '15', '00', '000', '18', '00', '000', 'benefit', 'job', 'type', 'full', 'time', 'remot', 'hybrid', 'option']\n",
      "\n",
      "Resume Match Percentage: 61.18 %\n",
      "\n",
      "Final Resume Similarity Score: 61.18 %\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import warnings\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Download required NLTK resources silently\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "# Function to preprocess text and display steps\n",
    "def preprocess_text(text):\n",
    "    print(\"\\nOriginal Text:\\n\", text)\n",
    "    \n",
    "    # Remove special characters\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text.lower())  \n",
    "    print(\"\\nTokenization:\\n\", tokens)\n",
    "    \n",
    "    # Stopword Removal\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    print(\"\\nAfter Stopword Removal:\\n\", filtered_tokens)\n",
    "    \n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "    print(\"\\nAfter Stemming:\\n\", stemmed_tokens)\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word, pos='v') for word in stemmed_tokens]\n",
    "    print(\"\\nAfter Lemmatization:\\n\", lemmatized_tokens)\n",
    "\n",
    "    return \" \".join(lemmatized_tokens)  # Return as a string for vectorization\n",
    "\n",
    "\n",
    "# Function to calculate similarity using Cosine Similarity\n",
    "def calculate_resume_match(resume_text, job_description):\n",
    "    # Preprocess both texts\n",
    "    resume_processed = preprocess_text(resume_text)\n",
    "    job_processed = preprocess_text(job_description)\n",
    "\n",
    "    # Convert text into TF-IDF vectors\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectors = vectorizer.fit_transform([resume_processed, job_processed])\n",
    "\n",
    "    # Compute Cosine Similarity\n",
    "    similarity_score = cosine_similarity(vectors[0], vectors[1])[0][0]\n",
    "\n",
    "    # Convert similarity to percentage\n",
    "    similarity_percentage = round(similarity_score * 100, 2)\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\nResume Match Percentage:\", similarity_percentage, \"%\")\n",
    "\n",
    "    return similarity_percentage\n",
    "\n",
    "\n",
    "# Call function\n",
    "match_percentage = calculate_resume_match(resume_text, job_description_1)\n",
    "print(\"\\nFinal Resume Similarity Score:\", match_percentage, \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ead92d9-5288-4827-a376-df40f5e814ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8772a4-2da8-467a-bd23-df33499de0e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
